{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "6394efd7_6bfa10a5",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-12T04:28:26Z",
      "side": 1,
      "message": "In real scenarios, connect can definitely take more than a second, especially on low links, or SSL that takes a long time to bring the link up, so 1 second isn\u0027t going to work.\n\nWhat I\u0027d like to understand is why this matters at all.  If the destination is unreachable, aren\u0027t we bound by the linux timeouts, and shouldn\u0027t those be relatively quick based on L2 network routing?  I would expect the timeout in this case is just a security precaution for compromised network nodes that might not bring up the full connection quickly in an attempt to run the bmc out of resources, but clearly you\u0027re seeing something else here.",
      "range": {
        "startLine": 10,
        "startChar": 17,
        "endLine": 11,
        "endChar": 26
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c443fad7_5f92defc",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1001303
      },
      "writtenOn": "2022-07-12T16:07:44Z",
      "side": 1,
      "message": "\u003e In real scenarios, connect can definitely take more than a second, especially on low links, or SSL that takes a long time to bring the link up, so 1 second isn\u0027t going to work.\nIn that case, is it possible to reduce the 30 second limit at all?\n\n\u003e ...but clearly you\u0027re seeing something else here\nOn my BMC I\u0027m seeing the timer in doConnect() completely expire when the destination is unreachable (i.e. \"The socket was closed due to a timeout\").  This takes the full 30 seconds.\n\nIn this case I\u0027m using an external IP rather than an arbitrary port on the localhost that has been configured to forward to a non-existent satellite BMC.",
      "parentUuid": "6394efd7_6bfa10a5",
      "range": {
        "startLine": 10,
        "startChar": 17,
        "endLine": 11,
        "endChar": 26
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "540fca60_d3e7bd42",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-12T18:43:57Z",
      "side": 1,
      "message": "1 is way too short.....  5-15 seconds would be in my head as a relatively sane default, but.... we should be looking to something with more industry traction for inspiration.  What does curl/wget/requests do in this case for a timeout, and can we just copy them?\n\nNow that I\u0027ve had my coffee, I\u0027m starting to understand the problem more;  You have to bring up the connection, and do the request in the time that it takes the primary request to time out, which means you have more constraints on your timing.\n\n(thinking out loud) What if we relied on keep alive, and just kept an open connection to each aggregated resource all the time, then, if we get a request we could immediately check if the connection exists, and 502 immediately, instead of waiting for connect to fail?",
      "parentUuid": "c443fad7_5f92defc",
      "range": {
        "startLine": 10,
        "startChar": 17,
        "endLine": 11,
        "endChar": 26
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b925826c_d74b8db5",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1001303
      },
      "writtenOn": "2022-07-12T21:08:34Z",
      "side": 1,
      "message": "\u003e What does curl/wget/requests do in this case for a timeout, and can we just copy them?\nBased on running curl on my local machine to some made-up destination, curl will just run until it hits its own internal timeout.  On my machine that seems to take ~130 seconds:\n```\ncurl: (28) Failed to connect to 192.92.32.1 port 80 after 130994 ms: Connection timed out\ncurl: (28) Failed to connect to 192.92.32.1 port 80 after 130045 ms: Connection timed out\ncurl: (28) Failed to connect to 192.92.32.1 port 80 after 129363 ms: Connection timed out\n```\n\nBasically we need to prove a negative (the destination does not exist) and the standard approach seems to be to wait for a timeout.  For now the only time that this hurts is the redfish aggregation edge case where either a satellite is unreachable, or the satellite config is incorrect.\n\n\u003e (thinking out loud) What if we relied on keep alive, and just kept an open connection to each aggregated resource all the time, then, if we get a request we could immediately check if the connection exists, and 502 immediately, instead of waiting for connect to fail?\n(Also thinking out loud) I think that would only work if the destination supports keep alive.  Also, I would need some way to handle the scenario where the destination blinking in and out results in not all of the connections in the pool remaining open.  \n\nI could take the same idea and add a bool to each connection pool that denotes if a connection timeout previously occurred.  That bool could only be set if we fall through due to a connection timeout.  It would be cleared anytime a connection is able to receive a response to a request.\n\nAs part of each message sendinging attempt we would first check the value of that bool for the specified connection pool.  When the bool is set then we would immediately return a 502, but at the same time we would also just send a ping to the destination to check in the background if that destination has become available again.  If it did then we would flip the bool so that future requests know to not return a 502.",
      "parentUuid": "540fca60_d3e7bd42",
      "range": {
        "startLine": 10,
        "startChar": 17,
        "endLine": 11,
        "endChar": 26
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "36384145_101b604c",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-12T22:11:42Z",
      "side": 1,
      "message": "\u003e \u003e What does curl/wget/requests do in this case for a timeout, and can we just copy them?\n\u003e Based on running curl on my local machine to some made-up destination, curl will just run until it hits its own internal timeout.  On my machine that seems to take ~130 seconds:\n\u003e ```\n\u003e curl: (28) Failed to connect to 192.92.32.1 port 80 after 130994 ms: Connection timed out\n\u003e curl: (28) Failed to connect to 192.92.32.1 port 80 after 130045 ms: Connection timed out\n\u003e curl: (28) Failed to connect to 192.92.32.1 port 80 after 129363 ms: Connection timed out\n\u003e ```\n\u003e \n\u003e Basically we need to prove a negative (the destination does not exist) and the standard approach seems to be to wait for a timeout.  For now the only time that this hurts is the redfish aggregation edge case where either a satellite is unreachable, or the satellite config is incorrect.\n\u003e \n\u003e \u003e (thinking out loud) What if we relied on keep alive, and just kept an open connection to each aggregated resource all the time, then, if we get a request we could immediately check if the connection exists, and 502 immediately, instead of waiting for connect to fail?\n\u003e (Also thinking out loud) I think that would only work if the destination supports keep alive. \n\nGood point, although it seems pretty reasonable that DMTF might require keepalive, given they require TLS.  I don\u0027t know of an implementation that doesn\u0027t support it, although you are technically correct.  For this use case it might be worth getting into the standard.\n\n\u003e Also, I would need some way to handle the scenario where the destination blinking in and out results in not all of the connections in the pool remaining open.  \n\n\nNot following in this case.  if tcp is dropping, wouldn\u0027t the connections be closing and be destroyed?\n\n\u003e \n\u003e I could take the same idea and add a bool to each connection pool that denotes if a connection timeout previously occurred.  That bool could only be set if we fall through due to a connection timeout.  It would be cleared anytime a connection is able to receive a response to a request.\n\n\n\u003e \n\u003e As part of each message sendinging attempt we would first check the value of that bool for the specified connection pool.  When the bool is set then we would immediately return a 502, but at the same time we would also just send a ping to the destination to check in the background if that destination has become available again.  If it did then we would flip the bool so that future requests know to not return a 502.\n\n\nOh, so you\u0027re thinking we keep connection objects alive regardless of their connectedness.  That might work?  Or maybe we need a \"last time connected\" time, so we can still handle the first-connect case?  Or the case where a keepalive was rightly timed out by the client.",
      "parentUuid": "b925826c_d74b8db5",
      "range": {
        "startLine": 10,
        "startChar": 17,
        "endLine": 11,
        "endChar": 26
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9a383535_abae737c",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1001303
      },
      "writtenOn": "2022-07-12T22:57:53Z",
      "side": 1,
      "message": "\u003e Good point, although it seems pretty reasonable that DMTF might require keepalive, given they require TLS.  I don\u0027t know of an implementation that doesn\u0027t support it, although you are technically correct.  For this use case it might be worth getting into the standard.\nI didn\u0027t consider that it\u0027s part of the standard.  If it is then I\u0027d feel better about relying on it.  However, there\u0027s still an issue with tracking this on a per connection basis.  I\u0027ll explain that later in this comment.\n\n\u003e \u003e Also, I would need some way to handle the scenario where the destination blinking in and out results in not all of the connections in the pool remaining open.\n\u003e Not following in this case.  if tcp is dropping, wouldn\u0027t the connections be closing and be destroyed?\nThey would be closed but they are not be destroyed.  Even then, a single connection fail would not affect the entire pool.  Let\u0027s say we have a full connection pool that has been sending w/o issue.  All of a sudden the destination goes down.  Each individual connection will still appear as idle until we attempt to send a message to the unreachable destination using that connection.  Similarly, if the destination comes back online then we risk false positives where the individual connections will each return a 502 the first time they are used.\n\n\u003e Oh, so you\u0027re thinking we keep connection objects alive regardless of their connectedness. That might work?\n(Provided I\u0027m understanding what keeping it alive means) That\u0027s effectively how the code functions now since there is not a method to delete a connection from the pool.  They get closed if something goes wrong, but they still remain in the pool\u0027s vector of connections.\n\n\u003e Or maybe we need a \"last time connected\" time, so we can still handle the first-connect case?  Or the case where a keepalive was rightly timed out by the client.\nWhen the pool is first created we can just set the bool to denote that there are no connection issues.  The first message send attempt would set the actual connection status.\n\nIf we want to make sure the destination is available before the first message send attempt then we\u0027d need to just send a dummy message whenever the other entity is created.  For Redfish Aggregation for example I\u0027d want to send a dummy message within the Constructor for RedfishAggregator to each satellite BMC.",
      "parentUuid": "36384145_101b604c",
      "range": {
        "startLine": 10,
        "startChar": 17,
        "endLine": 11,
        "endChar": 26
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "89392661_e44f531a",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T15:05:52Z",
      "side": 1,
      "message": "So the above kind of implies that we need some tracking of the \"health\" of a given remote that\u0027s separate from the connection itself (this is starting to get messy).\n\n(again thinking out loud) What if we used GET /redfish/v1 as a canary in the coal mine sort of set up?  If the system is up, it should definitely respond to /redfish/v1, and it\u0027s a response that doesn\u0027t really take up bandwidth.  (technically we could also do HEAD if we were really worried about bw).  If the system ever drops connections, we start trying to connect to /redfish/v1 at some rate, and 502 any incoming connection until we get at least one connection that succeeds in connecting to /redfish/v1.  Because our /redfish/v1 polling task is local and not tied to a response, it can take as long as it likes to time out (or we can set an arbitrarily short timeout), and when the resource goes down, we could immediately 502 instead of waiting for a timeout.\n\n\nI wonder how nginx deals with this.  It has a reverse proxy and seems to time out in a reasonable amount of time.",
      "parentUuid": "9a383535_abae737c",
      "range": {
        "startLine": 10,
        "startChar": 17,
        "endLine": 11,
        "endChar": 26
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "7e07824b_7327f7e9",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1001303
      },
      "writtenOn": "2022-07-13T15:47:07Z",
      "side": 1,
      "message": "\u003e So the above kind of implies that we need some tracking of the \"health\" of a given remote that\u0027s separate from the connection itself (this is starting to get messy).\nYes and absolutely yes\n\n\u003e What if we used GET /redfish/v1 as a canary in the coal mine sort of set up?\nYeah I think that\u0027s a good option.\n\n\u003e Because our /redfish/v1 polling task is local and not tied to a response, it can take as long as it likes to time out (or we can set an arbitrarily short timeout)\nSounds reasonable.\n\nWould it be alright to also poll the destination each time we attempt to send to a currently 502 destination rather than waiting for the polling task to update it again?\n\nI suspect as a follow up to this we\u0027ll also want to add some sort of method to remove connection pools.  For example, if we delete a subscription then we\u0027d want to delete its connection pool if it\u0027s not being used by another subscription or Redfish Aggregation.  No sense in polling unavailable destinations that the system won\u0027t be using.",
      "parentUuid": "89392661_e44f531a",
      "range": {
        "startLine": 10,
        "startChar": 17,
        "endLine": 11,
        "endChar": 26
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4b47f701_861bd49f",
        "filename": "http/http_client.hpp",
        "patchSetId": 4
      },
      "lineNbr": 358,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-12T04:28:26Z",
      "side": 1,
      "message": "Now that this doesn\u0027t return early, this does nothing (which cppcheck will flag)",
      "range": {
        "startLine": 358,
        "startChar": 0,
        "endLine": 358,
        "endChar": 43
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "4bd45576_18c407ec",
        "filename": "http/http_client.hpp",
        "patchSetId": 4
      },
      "lineNbr": 358,
      "author": {
        "id": 1001303
      },
      "writtenOn": "2022-07-12T16:07:44Z",
      "side": 1,
      "message": "I removed it",
      "parentUuid": "4b47f701_861bd49f",
      "range": {
        "startLine": 358,
        "startChar": 0,
        "endLine": 358,
        "endChar": 43
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "db3f47b8_4b9fc75f",
        "filename": "http/http_client.hpp",
        "patchSetId": 4
      },
      "lineNbr": 382,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-12T04:28:26Z",
      "side": 1,
      "message": "Same here.",
      "range": {
        "startLine": 382,
        "startChar": 0,
        "endLine": 382,
        "endChar": 43
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "24f963b3_9d358fd1",
        "filename": "http/http_client.hpp",
        "patchSetId": 4
      },
      "lineNbr": 382,
      "author": {
        "id": 1001303
      },
      "writtenOn": "2022-07-12T16:07:44Z",
      "side": 1,
      "message": "I removed it",
      "parentUuid": "db3f47b8_4b9fc75f",
      "range": {
        "startLine": 382,
        "startChar": 0,
        "endLine": 382,
        "endChar": 43
      },
      "revId": "122f379bf286075a7aac335399f2d1fb28765c15",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    }
  ]
}